# -*- coding: utf-8 -*-
"""Webscrape_SO_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wxApt10ftHocO_QVZSo0sHAeXqEiblHL
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import requests # Getting Webpage content
from bs4 import BeautifulSoup as bs # Scraping webpages
import matplotlib.pyplot as plt # Visualization
import matplotlib.style as style # For styling plots
from matplotlib import pyplot as mp # For Saving plots as images

# For displaying plots in jupyter notebook
# %matplotlib inline 

style.use('fivethirtyeight') # matplotlib Style

# Using requests module for downloading webpage content
response = requests.get('https://stackoverflow.com/search?page=1&tab=Relevance&q=android%20user%20data')

# Getting status of the request
# 200 status code means our request was successful
# 404 status code means that the resource you were looking for was not found
response.status_code

# Parsing html data using BeautifulSoup
soup = bs(response.content, 'html.parser')

# body 
body = soup.find('body')

# printing the object type of body
type(body)
type(soup)

lang_tags = body.find_all('a', class_='post-tag')
lang_tags[:2]

tags = [i.text for i in lang_tags]
tags[:5]

# Selecting a tags inside p tag
soup.select('p a')

# Using requests module for downloading webpage content
response1 = requests.get('https://stackoverflow.com/search?page=1&tab=Relevance&q=android%20user%20data')

# Getting status of the request
# 200 status code means our request was successful
# 404 status code means that the resource you were looking for was not found
response1.status_code

# Parsing html data using BeautifulSoup
soup1 = bs(response1.content, 'html.parser')

# body 
body1 = soup1.select_one('body')

# printing the object type of body
type(body1)

"""##Extracting Date Q Asked
###(verified)
"""

date_tags = body1.findAll('span', class_='relativetime')
for date_tag in date_tags:
  print(date_tag.text)

#question_links = body1.select("h3 a.question-hyperlink")
#question_links[:2]

question_list=[]

for a_tag in soup.find_all('a', class_='question-hyperlink', href=True):
    question_list.append(a_tag['href'])

question_list[:14]
#len(question_list)

r = requests.get("https://stackoverflow.com"+question_list[2])
soup = bs(r.content)
r.status_code

"""##Extracting Title
###(verified)
"""

title_tag = soup.find('h1', class_='grid--cell fs-headline1 fl1 ow-break-word mb8')
title_tag.string

"""##Extracting Summary
###(verified)
"""

summary_tag = soup.find('div', class_='post-text')
summary_tag.text

"""##Extracting Votes
###(verified)
"""

vote_tag = soup.find('div', class_='js-vote-count')
vote_tag.text

"""##Extracting Views
###(verified)
"""

view_tag = soup.find('div', class_='grid--cell ws-nowrap mb8')
view_tag.text.strip()

"""##Extracting tags
###(verified)
"""

tag_tag = soup.find('div', class_='grid ps-relative d-block')
tag_tag.text.strip()

"""##Extracting Accepted Answers
###(verified)
"""

#answer text
accepted_ans = soup.find('div', class_ = 'answer accepted-answer')
accepted_ans_text = accepted_ans.find('div', class_= 'post-text')
accepted_ans_text.text

#number of votes
accepted_ans_upvotes = accepted_ans.find('div', class_ = 'js-vote-count', itemprop = 'upvoteCount')
accepted_ans_votes = int(accepted_ans_upvotes.text)
accepted_ans_votes

"""##Extracting Most Voted Answer
###(verified)
"""

#answer text
voted_ans = soup.find('div', class_ = 'answer', itemprop = 'suggestedAnswer')
voted_ans_text = voted_ans.find('div', class_= 'post-text')
voted_ans_text.text

#number of votes
voted_ans_upvotes = voted_ans.find('div', class_ = 'js-vote-count', itemprop = 'upvoteCount')
voted_ans_votes = int(voted_ans_upvotes.text)
voted_ans_votes

"""##Conditions for answers
###(verified)
"""

#Accepted answer
#answer text
accepted_ans = soup.find('div', class_ = 'answer accepted-answer')
if(accepted_ans is not None):
  accepted_ans_text = accepted_ans.find('div', class_= 'post-text')
  accepted_ans_text.text

  #number of votes
  accepted_ans_upvotes = accepted_ans.find('div', class_ = 'js-vote-count', itemprop = 'upvoteCount')
  accepted_ans_votes = int(accepted_ans_upvotes.text)
  accepted_ans_votes


#Most voted answer
#answer text
voted_ans = soup.find('div', class_ = 'answer', itemprop = 'suggestedAnswer')
if(voted_ans is not None):
  voted_ans_text = voted_ans.find('div', class_= 'post-text')
  voted_ans_text.text

  #number of votes
  voted_ans_upvotes = voted_ans.find('div', class_ = 'js-vote-count', itemprop = 'upvoteCount')
  voted_ans_votes = int(voted_ans_upvotes.text)
  voted_ans_votes

"""##Timer"""

import time
print('Rishi')
time.sleep(2)
print('Dhaka')

"""##Moving to next page
###(verified)
"""

#scroll_tags = body.find('a', class_='s-pagination--item js-pagination-item', rel_='next')
scroll_tags = body.find('div', class_='s-pagination pager fl')
next_tag = scroll_tags.find('a', rel='next')
next_tag = scroll_tags.find('a', rel='next', href=True)['href']
print(type(next_tag))
rnext = requests.get("https://stackoverflow.com"+next_tag)
rnext.status_code

# Parsing html data using BeautifulSoup
soup_next = bs(rnext.content, 'html.parser')

# body 
body_next = soup_next.select_one('body')

"""##Question list extraction
### (verified)
"""

questions = body_next.find_all('div', class_='question-summary search-result')
question_links = []
for q in questions:
  question_links.append(q.find('a',class_='question-hyperlink', href=True)['href'])

question_links

"""#Merger"""

webscrape("https://stackoverflow.com/search?q=apple+application+data")

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import requests # Getting Webpage content
from bs4 import BeautifulSoup as bs # Scraping webpages
import matplotlib.pyplot as plt # Visualization
import matplotlib.style as style # For styling plots
from matplotlib import pyplot as mp # For Saving plots as images

# For displaying plots in jupyter notebook
# %matplotlib inline 

style.use('fivethirtyeight') # matplotlib Style


#declaring variables
date_list = []
title, summary, votes, views, tags, accepted_answer, voted_answer = [], [], [], [], [], [], []



#declaring function
def webscrape(website):

  # Using requests module for downloading webpage content
  response = requests.get(website)

  # Getting status of the request
  # 200 status code means our request was successful
  # 404 status code means that the resource you were looking for was not found
  print(response.status_code)



  # Parsing html data using BeautifulSoup
  soup = bs(response.content, 'html.parser')

  # body 
  body = soup.find('body')



  #Extracting dates for each question posted
  date_tags = body.findAll('span', class_='relativetime')
  for date_tag in date_tags:
    date_list.append(date_tag.text)



  #Extracting question links (href)
  questions = body.find_all('div', class_='question-summary search-result')
  question_links = []
  for q in questions:
    question_links.append(q.find('a',class_='question-hyperlink', href=True)['href'])



  #Opening url of each question
  for i in question_links:
    r = requests.get("https://stackoverflow.com"+i)
    soup = bs(r.content)
    #print(r.status_code)



    #extracting title of the question
    title_tag = soup.find('h1', class_='grid--cell fs-headline1 fl1 ow-break-word mb8')
    title.append(title_tag.string.strip())


    #extracting summary
    summary_tag = soup.find('div', class_='post-text')
    summary.append(summary_tag.text.strip())


    #extracting votes
    vote_tag = soup.find('div', class_='js-vote-count')
    votes.append(vote_tag.text.strip())


    #extracting views
    view_tag = soup.find('div', class_='grid--cell ws-nowrap mb8')
    views.append(view_tag.text.strip())


    #extracting tags
    tag_tag = soup.find('div', class_='grid ps-relative d-block')
    if(tag_tag):
      tags.append(tag_tag.text.strip())
    else:
      tags.append('')


    #extracting Accepted answer
    #answer text
    accepted_ans = soup.find('div', class_ = 'answer accepted-answer')
    if(accepted_ans is not None):
      accepted_ans_text = accepted_ans.find('div', class_= 'post-text')
      accepted_ans_text = accepted_ans_text.text

      #number of votes
      accepted_ans_upvotes = accepted_ans.find('div', class_ = 'js-vote-count', itemprop = 'upvoteCount')
      accepted_ans_votes = int(accepted_ans_upvotes.text)
    
    else:
      accepted_ans_text = 'No Answer found'
      accepted_ans_votes = -1000

    accepted_answer.append(accepted_ans_text)


    #Most voted answer
    #answer text
    voted_ans = soup.find('div', class_ = 'answer', itemprop = 'suggestedAnswer')
    if(voted_ans is not None):
      voted_ans_text = voted_ans.find('div', class_= 'post-text')
      voted_ans_text = voted_ans_text.text      

      #number of votes
      voted_ans_upvotes = voted_ans.find('div', class_ = 'js-vote-count', itemprop = 'upvoteCount')
      voted_ans_votes = int(voted_ans_upvotes.text)
    
    else:
      voted_ans_votes = -1000
    

    #Conditions for appending answers
    if(voted_ans_votes > accepted_ans_votes):
      voted_answer.append(voted_ans_text)
    else:
      voted_answer.append(accepted_ans_text)

    #timer
    import time
    time.sleep(4)



  #moving to next page
  scroll_tags = body.find('div', class_='s-pagination pager fl')
  next_tag = scroll_tags.find('a', rel='next')
  next_tag = scroll_tags.find('a', rel='next', href=True)['href']
  if isinstance(next_tag, str):
    webscrape("https://stackoverflow.com"+next_tag)

  #for j in range(0, 15):
  #  print(date_list[j], ' ', title[j], ' ', 'summary[j]', ' ', views[j], ' ', votes[j], ' ', tags[j])

i = len(date_list)
print(i)

import csv

csv1 = open('apple_application_data.csv', 'w', newline='')
csv1_writer = csv.writer(csv1)
csv1_writer.writerow(date_list)
csv1_writer.writerow(title)
csv1_writer.writerow(summary)
csv1_writer.writerow(views)
csv1_writer.writerow(votes)
csv1_writer.writerow(tags)
csv1_writer.writerow(accepted_answer)
csv1_writer.writerow(voted_answer)

